{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sw6820/wanted_raw/blob/main/raw_Week2_2_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592U6lXs3d2t"
      },
      "source": [
        "# Week2_2 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- \"네이버 영화 감성 분류\" 데이터를 불러와 `pandas` 라이브러리를 사용해 **전처리** 할 수 있다.\n",
        "- 적은 데이터로도 높은 성능을 내기 위해, pre-trained `BERT` 모델 위에 1개의 hidden layer를 쌓아 **fine-tuning**할 수 있다.\n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- 토큰화된 학습 데이터를 배치 단위로 갖는 **traindata iterator**를 구현할 수 있다. \n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- **loss와 optimizer 함수**를 사용할 수 있다. \n",
        "- traindata iterator를 for loop 돌며 **fine-tuning** 할 수 있다.\n",
        "- fine-tuning의 2가지 방법론을 비교할 수 있다. \n",
        "  - BERT 파라미터를 **freeze** 한 채 fine-tuning (Vision에서 주로 사용하는 방법론)\n",
        "  - BERT 파라미터를 **unfreeze** 한 채 fine-tuning (NLP에서 주로 사용하는 방법론)\n",
        "\n",
        "\n",
        "### Reference\n",
        "- [huggingface 한국어 오픈소스 모델](https://huggingface.co/models?language=ko&sort=downloads&search=bert)\n",
        "- [transformer BertForSequenceClassification 소스 코드](https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert/modeling_bert.py#L1501)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSX-wQA1RD1h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import torch\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Reyt-HvLnJv"
      },
      "outputs": [],
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUR6vb3L3d2u",
        "outputId": "acdeef63-2af7-449b-92c4-a5f88c9bf423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "  print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c93M8XmjLnJw"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0REKl4EvT9G1"
      },
      "source": [
        "### 데이터 다운로드 및 DataFrame 형태로 불러오기\n",
        "- 내 구글 드라이브에 데이터를 다운받은 후 코랩에 드라이브를 마운트하면 데이터를 영구적으로 사용할 수 있음.\n",
        "- [네이버영화감성분류](https://github.com/e9t/nsmc)\n",
        "  - trainset: 150,000 \n",
        "  - testset: 50,000 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lEWUggR1R9rS",
        "outputId": "d94d060a-4b29-42b9-a68d-6da6029ddde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rov1s8IxSLqy",
        "outputId": "e6b63683-5c1e-4373-ceed-0d1aec5471f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/여기에 파일 경로를 입력'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/MyDrive/여기에 파일 경로를 입력\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OjPGnbEjVYmj",
        "outputId": "40fdd879-c452-4713-fe8b-2442437451c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nsmc'...\n",
            "remote: Enumerating objects: 14763, done.\u001b[K\n",
            "remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n",
            "Receiving objects: 100% (14763/14763), 56.19 MiB | 18.85 MiB/s, done.\n",
            "Resolving deltas: 100% (1749/1749), done.\n",
            "Checking out files: 100% (14737/14737), done.\n"
          ]
        }
      ],
      "source": [
        "# 데이터 다운로드\n",
        "!git clone https://github.com/e9t/nsmc.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SueG9v14YbgF",
        "outputId": "39a36653-0d24-4075-e466-5d9c7157f74e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My current directory : /content\n"
          ]
        }
      ],
      "source": [
        "_CUR_DIR = os.path.abspath(os.curdir)\n",
        "print(f\"My current directory : {_CUR_DIR}\")\n",
        "_DATA_DIR = os.path.join(_CUR_DIR, \"nsmc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J6KQ8dzaHBi"
      },
      "outputs": [],
      "source": [
        "# nsmc/ratings_train.txt를 DataFrame 형태로 불러오기\n",
        "df = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cUsoBEPahlo"
      },
      "outputs": [],
      "source": [
        "# 데이터 크기 확인\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic3k9CORaXzM"
      },
      "outputs": [],
      "source": [
        "# 데이터 일부 확인\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA1F0tHWLnJz"
      },
      "source": [
        "### 데이터 결측치 제거 및 데이터 수 줄이기 \n",
        "- 학습 데이터 수는 150,000개로 매우 많은 양이다. 하지만 우리가 실생활에서 마주할 데이터는 이렇게 많지 않다. 이 때 유용하게 사용되는 것이 **fine-tuning** 학습 방법이다.   \n",
        "- Fine-tuning은 단어의 의미를 이미 충분히 학습한 모델 (여기서는 **BERT**)을 가져와 그 위에 추가적인 Nueral Network 레이어를 쌓은 후 학습하는 방법론이다. 이미 BERT가 단어의 의미를 충분히 학습했기 때문에 **적은 데이터**로 학습해도 우수한 성능을 낼 수 있다는 장점이 있다. \n",
        "- **데이터의 label의 비율이 5:5를 유지하면서** 학습 데이터 수를 150,000개에서 1,000개로 줄이\b는 함수 `label_evenly_balanced_dataset_sampler`를 구현하라.\n",
        "  - 함수 정의 \n",
        "    - 입력 매개변수\n",
        "      - df : DataFrame\n",
        "      - n_sample : df에서 샘플링할 row의 개수 (여기서는 1000개로 정의한다)\n",
        "    - 조건\n",
        "      - label의 비율이 5:5를 유지할 수 있도록 샘플링한다.\n",
        "    - 반환값\n",
        "      - row의 개수가 1000개인 dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh9BSiSeMms7"
      },
      "outputs": [],
      "source": [
        "# df에서 결측치 (na 값) 제거\n",
        "\n",
        "df = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ommF5KH4akCJ"
      },
      "outputs": [],
      "source": [
        "# label별 데이터 수 확인\n",
        "# pandas의 value_counts 함수 활용\n",
        "# 0 -> 부정 1 -> 긍정\n",
        "\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ii06wCsc107"
      },
      "outputs": [],
      "source": [
        "# 학습 데이터 샘플 개수 설정\n",
        "\n",
        "n_sample = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hrkhl69Dc-kr"
      },
      "outputs": [],
      "source": [
        "# 샘플링 함수 구현\n",
        "# random 모듈에서 제공되는 함수 활용\n",
        "# input: 학습 데이터 샘플 개수\n",
        "# output: 샘플링 데이터\n",
        "\n",
        "\n",
        "def label_evenly_balanced_dataset_sampler(df, sample_size):\n",
        "  \"\"\"\n",
        "  데이터 프레임의을 sample_size만큼 임의 추출해 새로운 데이터 프레임을 생성.\n",
        "  이 때, \"label\"열의 값들이 동일한 비율을 갖도록(5:5) 할 것.\n",
        "  \"\"\"\n",
        "\n",
        "  sample = None\n",
        "\n",
        "  return sample\n",
        "\n",
        "sample_df = label_evenly_balanced_dataset_sampler(df, n_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXLT6tAdaA34"
      },
      "outputs": [],
      "source": [
        "# 검증\n",
        "\n",
        "sample_df.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLNUjgawLnJ1"
      },
      "source": [
        "### CustomClassifier 클래스 구현\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week2/week2_2_bertclf.png?raw=true\" width=400>\n",
        "\n",
        "- 그림과 같이 사전 학습(pre-trained)된 `BERT` 모델을 불러와 그 위에 **1 hidden layer**와 **binary classifier layer**를 쌓아 fine-tunning 모델을 생성할 것이다.    \n",
        "---\n",
        "- hidden layer 1개와 output layer(binary classifier layer)를 갖는 `CustomClassifier` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "  - 생성자 입력 매개변수\n",
        "    - `hidden_size` : BERT의 embedding size\n",
        "    - `n_label` : class(label) 개수\n",
        "  - 생성자에서 생성할 변수\n",
        "    - `bert` : BERT 모델 인스턴스 \n",
        "    - `classifier` : 1 hidden layer + relu +  dropout + classifier layer를 stack한 `nn.Sequential` 모델\n",
        "      - 첫번재 히든 레이어 (첫번째 `nn.Linear`)\n",
        "        - input: BERT의 마지막 layer의 1번재 token ([CLS] 토큰) (shape: `hidden_size`)\n",
        "        - output: (shape: `linear_layer_hidden_size`)\n",
        "      - 아웃풋 레이어 (두번째 `nn.Linear`)\n",
        "        - input: 첫번째 히든 레이어의 아웃풋 (shape: `linear_layer_hidden_size`)\n",
        "        - output: target/label의 개수 (shape:2)\n",
        "  - 메소드\n",
        "    - `forward()`\n",
        "      - BERT output에서 마지막 레이어의 첫번째 토큰 ('[CLS]')의 embedding을 가져와 `self.classifier`에 입력해 아웃풋으로 logits를 출력함.\n",
        "  - 주의 사항\n",
        "    - `CustomClassifier` 클래스는 부모 클래스로 `nn.Module`을 상속 받는다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0WbqVv62Zvy"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im98H4-U1eQQ"
      },
      "outputs": [],
      "source": [
        "# classifier 구현\n",
        "class CustomClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size: int, n_label: int):\n",
        "    super(CustomClassifier, self).__init__()\n",
        "\n",
        "    self.bert = BertModel.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "    dropout_rate = 0.1\n",
        "    linear_layer_hidden_size = 32\n",
        "\n",
        "    self.classifier = nn.Sequential(None) # torch.nn에서 제공되는 Sequential, Linear, ReLU, Dropout 함수 활용\n",
        "\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "\n",
        "    outputs = self.bert(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids,\n",
        "    )\n",
        "\n",
        "    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n",
        "    cls_token_last_hidden_states = \bout['pooler_output'] # 마지막 layer의 첫 번째 토큰 (\"[CLS]\") 벡터를 가져오기, shape = (1, hidden_size)\n",
        "\n",
        "    logits = self.classifier(cls_token_last_hidden_states)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x7PU1t1LnJ1"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXesCG5TLnJ1"
      },
      "source": [
        "### 학습 데이터를 배치 단위로 저장하는 이터레이터 함수 `data_iterator` 구현\n",
        "- 데이터 프레임을 입력 받아 text를 \b토큰 id로 변환하고 label은 텐서로 변환해 배치만큼 잘라 (input, \btarget) 튜플 형태의 이터레이터를 생성하는 `data_iterator` 함수를 구현하라.\n",
        "- 함수 정의 \n",
        "  - 입력 매개변수\n",
        "    - `input_column` : text 데이터 column 명\n",
        "    - `target_column` : label 데이터 column 명\n",
        "    -  `batch_size` : 배치 사이즈\n",
        "  - 조건\n",
        "    - 함수는 다음을 수행해야 함 \n",
        "      - 데이터 프레임 랜덤 셔플링\n",
        "      - `tokenizer_bert`로 text를 token_id로 변환 + 텐서화 \n",
        "      - target(label)을 텐서화\n",
        "  - 반환값 \n",
        "    - (input, target) 튜플 형태의 이터레이터를 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-tJERGI4Fzk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlcYCOyW3d2t"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_U_c-Mf3d2t"
      },
      "outputs": [],
      "source": [
        "tokenizer_bert = BertTokenizer.from_pretrained(\"klue/bert-base\") # lower-cased version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2VnIY-ALnJ2"
      },
      "outputs": [],
      "source": [
        "# 토크나이징 예시 (1개의 문장)\n",
        "\n",
        "# 1. string type의 문장을 가져옴\n",
        "ex_sent = sample_df.document.iloc[0]\n",
        "print(f\"Original Sentence: {ex_sent}\\n\")\n",
        "\n",
        "# 2. 문장을 토크나이즈 함. 이 때, 특수 토큰 (\"[CLS]\", \"[SPE]\")을 자동으로 추가하고 pytorch의 tensor형태로 변환해 반환함\n",
        "tensor_sent = tokenizer_bert(\n",
        "    ex_sent,\n",
        "    add_special_tokens=True, # 문장의 앞에 문장 시작을 알리는 \"[CLS]\"토큰, 문장의 끝에 문장 끝을 알리는 \"[SPE]\"토큰을 자동으로 추가\n",
        "    return_tensors='pt' # pytorch tensor로 반환할 것\n",
        ")\n",
        "print(f\"Tokenized Sentence: {tensor_sent}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtXP_wRFLnJ2"
      },
      "outputs": [],
      "source": [
        "# 토크나이징 예시 (2개의 문장)\n",
        "\n",
        "# 1. 2개의 문장을 가진 list 생성\n",
        "ex_sent_list = list(sample_df.document.iloc[:2].values)\n",
        "for i, sent in enumerate(ex_sent_list):\n",
        "    print(f\"Original Sentence {i+1}: {sent}\")\n",
        "\n",
        "# 2. 문장 리스트를 토크나이즈 함. 이 때, 리스트 내 문장들의 토큰 길이가 동일할 수 있도록 가장 긴 문장을 기준으로 부족한 위치에 \"[PAD]\" 토큰을 추가\n",
        "tensor_sent_list = tokenizer_bert(\n",
        "    ex_sent_list,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors='pt',\n",
        "    padding=\"longest\" # 가장 긴 문장을 기준으로 token개수를 맞춤. 모자란 토큰 위치는 \"[PAD]\" 토큰을 추가\n",
        ")\n",
        "\n",
        "print(f\"\\nTokenized Sentence list: {tensor_sent_list}\")\n",
        "\n",
        "# 토크나이즈 된 두 문장의 길이가 동일함을 검증\n",
        "assert tensor_sent_list['input_ids'][0].shape == tensor_sent_list['input_ids'][1].shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR22xs-xf1QH"
      },
      "outputs": [],
      "source": [
        "def data_iterator(df, input_column, target_column, batch_size):\n",
        "  \"\"\"\n",
        "  데이터 프레임을 셔플한 후 \n",
        "  데이터 프레임의 input_column을 batch_size만큼 잘라 토크나이즈 + 텐서화하고, target_column을 batch_size만큼 잘라 텐서화 하여\n",
        "  (input, output) 튜플 형태의 이터레이터를 생성\n",
        "  \"\"\"\n",
        "\n",
        "  global tokenizer_bert\n",
        "\n",
        "  # 1. 데이터 프레임 셔플\n",
        "  #    pandas의 sample 함수 사용\n",
        "  df = None\n",
        "\n",
        "  # 2. 이터레이터 생성\n",
        "  for idx in range(0, df.shape[0], batch_size):\n",
        "    batch_df = None # batch_size만큼 데이터 추출\n",
        "    \n",
        "    tensorized_input = tokenizer_bert(None) # df의 text를 토크나이징 + token id로 변환 + 텐서화 (df의 input_column 사용)\n",
        "    \n",
        "    tensorized_target = None # target(label)을 텐서화 (df의 target_column 사용)\n",
        "    \n",
        "    yield tensorized_input, tensorized_target # 튜플 형태로 yield"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTlAV0hqILmc"
      },
      "outputs": [],
      "source": [
        "batch_size=32\n",
        "train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9VNAMchf1QI"
      },
      "outputs": [],
      "source": [
        "next(train_iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqnp2Q6ZLnJ2"
      },
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQVTqAUxLnJ2"
      },
      "source": [
        "### `data_iterator` 함수로 생성한 이터레이터를 for loop 돌면서 배치 단위의 데이터를 모델에 학습하는 `train()` 함수 구현\n",
        "- 함수 정의\n",
        "  - 입력 매개변수\n",
        "    - `model` : BERT + 1 hidden layer classifier 모델\n",
        "    - `data_iterator` : train data iterator\n",
        "- Reference\n",
        "  - [Loss: CrossEntropyLoss official document](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "  - [Optimizer: AdamW official document](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sE7xjYcRD1p"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from numpy.core.fromnumeric import nonzero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Er1qKtsf1QJ"
      },
      "outputs": [],
      "source": [
        "# 모델 클래스 정의\n",
        "model = CustomClassifier(hidden_size=768, n_label=2)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# 데이터 이터레이터 정의 \n",
        "train_iterator = None\n",
        "\n",
        "# 로스 및 옵티마이저\n",
        "loss_fct = CrossEntropyLoss()\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=2e-5,\n",
        "    eps=1e-8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvY5rxDKHQAp"
      },
      "outputs": [],
      "source": [
        "def train(model, data_iterator):\n",
        "\n",
        "  global loss_fct # 위에서 정의한 loss 함수\n",
        "\n",
        "  # 배치 단위 평균 loss와 총 평균 loss 계산하기위해 변수 생성\n",
        "  total_loss, batch_loss, batch_count = 0,0,0\n",
        "  \n",
        "  # model을 train 모드로 설정 & device 할당\n",
        "  model.train()\n",
        "  model.to(device)\n",
        "  \n",
        "  # data iterator를 돌면서 하나씩 학습\n",
        "  for step, batch in enumerate(data_iterator):\n",
        "    batch_count+=1\n",
        "    \n",
        "    # tensor 연산 전, 각 tensor에 device 할당\n",
        "    batch = tuple(item.to(device) for item in batch)\n",
        "    \n",
        "    batch_input, batch_label = batch\n",
        "    \n",
        "    # batch마다 모델이 갖고 있는 기존 gradient를 초기화\n",
        "    model.zero_grad()\n",
        "    \n",
        "    # forward\n",
        "    logits = None\n",
        "    \n",
        "    # loss\n",
        "    loss = loss_fct(None, None)\n",
        "    batch_loss += loss.item()\n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n",
        "    loss.backward()\n",
        "    \n",
        "    # optimizer 업데이트\n",
        "    optimizer.step()\n",
        "      \n",
        "    # 배치 10개씩 처리할 때마다 평균 loss를 출력\n",
        "    if (step % 10 == 0 and step != 0):\n",
        "      print(f\"Step : {step}, Avg Loss : {batch_loss / batch_count:.4f}\")\n",
        "      \n",
        "      # 변수 초기화 \n",
        "      batch_loss, batch_count = 0,0\n",
        "  \n",
        "  print(f\"Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "  print(\"Train Finished\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEYo8z9RLnJ2"
      },
      "source": [
        "### 지금까지 구현한 함수와 클래스를 모두 불러와 `train()` 함수를 실행하자\n",
        "- fine-tuning 모델 클래스 (`CustomClassifier`)\n",
        "    - hidden_size = 768\n",
        "    - n_label = 2\n",
        "- 데이터 이터레이터 함수 (`data_iterator`)\n",
        "    - batch_size = 32\n",
        "- loss \n",
        "    - `CrossEntropyLoss()`\n",
        "- optimizer\n",
        "    - optimizer는 loss(오차)를 상쇄하기 위해 파라미터를 업데이트 하는 과정\n",
        "    - `optimizer.step()` 시 파라미터가 업데이트 됨 \n",
        "    - lr = 2e-5\n",
        "- Reference\n",
        "  - [Optimizer 종류 설명 한국어 블로그 ](https://ganghee-lee.tistory.com/24)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCODIxCfFEDP"
      },
      "outputs": [],
      "source": [
        "# 학습 시작\n",
        "train(model, train_iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37UbAkh7LnJ3"
      },
      "source": [
        "## fine-tuning 2가지 방법론 비교\n",
        "- pre-trained BERT 모델 파라미터를 **freeze**한 채 학습하라\n",
        "    - BERT의 파라미터의 `requires_grad` 값을 `False`로 바꾸면, 학습 시 BERT의 파라미터는 미분이 계산되지도, 업데이트 되지도 않는다. \n",
        "    - 이렇게 특정 모델의 파라미터가 업데이트 하지 못하도록 설정하는 것을 **freeze**라고 한다. \n",
        "    - BERT 파라미터를 freeze시킨 채 학습을 진행해보자. 이럴 경우, 우리가 직접 쌓은 fine-tuning layer의 파라미터만 업데이트 된다. \n",
        "- **unfreeze**와 **freeze** 모델의 성능을 비교해 보자. 어떤 방식이 더 우수한가?\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld260K3YLnJ3"
      },
      "outputs": [],
      "source": [
        "class CustomClassifierFreezed(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size: int, n_label: int):\n",
        "    super(CustomClassifierFreezed, self).__init__()\n",
        "\n",
        "    self.bert = BertModel.from_pretrained(\"klue/bert-base\")\n",
        "    # freeze BERT parameter\n",
        "    # BERT의 파라미터는 고정값으로 두고 BERT 위에 씌운 linear layer의 파라미터만 학습하려고 한다. \n",
        "    # 이 경우, BERT의 파라미터의 'requires_grad' 값을 False로 변경해줘야 학습 시 해당 파라미터의 미분값이 계산되지 않는다.\n",
        "    for param in self.bert.parameters():\n",
        "        None\n",
        "\n",
        "    dropout_rate = 0.1\n",
        "    linear_layer_hidden_size = 32\n",
        "\n",
        "    self.classifier = nn.Sequential(None)\n",
        "\n",
        "      \n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "\n",
        "    outputs = self.bert(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids,\n",
        "    )\n",
        "    \n",
        "    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n",
        "    cls_token_last_hidden_states = \bout['pooler_output'] # 마지막 layer의 첫 번째 토큰 (\"[CLS]\") 벡터를 가져오기, shape = (1, hidden_size)\n",
        "    logits = self.classifier(cls_token_last_hidden_states)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ClEEHB6F6LW"
      },
      "outputs": [],
      "source": [
        "# freeze 모델\n",
        "# model을 제외한 설정값은 \b위에서 실행한 unfreeze 모델과 동일\n",
        "model = None\n",
        "\n",
        "# 데이터 이터레이터\n",
        "batch_size = None \n",
        "train_iterator = None\n",
        "\n",
        "# 로스 및 옵티마이저\n",
        "loss_fct = None\n",
        "optimizer = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqcPcWZeLnJ3"
      },
      "outputs": [],
      "source": [
        "# 학습 시작\n",
        "train(model, train_iterator)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "raw - Week2_2_assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}